//
//  SpeechRecognitionManager.swift
//  knowhow
//
//  Created by F1reC on 2025/7/25.
//

import Foundation
import Speech
import AVFoundation
import SwiftUI

class SpeechRecognitionManager: NSObject, ObservableObject {
    // ... (å·²æœ‰å±æ€§ä¿æŒä¸å˜)
    @Published var transcribedText = ""
    @Published var isRecording = false
    @Published var isAuthorized = false
    @Published var errorMessage = ""
    @Published var currentLanguage = "zh-CN"
    
    // --- æ–°å¢ï¼šç”¨äºUIå®æ—¶æ›´æ–°éŸ³é¢‘éŸ³é‡ ---
    @Published var audioLevel: Float = 0.0
    
    // ... (å·²æœ‰å±æ€§ä¿æŒä¸å˜)
    private let supportedLanguages = [
        "zh-CN": "ä¸­æ–‡(ç®€ä½“)",
        "en-US": "English",
        "zh-TW": "ä¸­æ–‡(ç¹ä½“)"
    ]
    private var speechRecognizers: [String: SFSpeechRecognizer] = [:]
    private var currentRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private var finalResults: [String] = []
    
    // --- ä¼˜åŒ–ï¼šåœ¨ init ä¸­è°ƒç”¨ setup ---
    override init() {
        super.init()
        setupSpeechRecognizers()
        requestSpeechAuthorization()
    }
    
    // åˆå§‹åŒ–å¤šè¯­è¨€è¯†åˆ«å™¨
    private func setupSpeechRecognizers() {
        for (languageCode, _) in supportedLanguages {
            if let recognizer = SFSpeechRecognizer(locale: Locale(identifier: languageCode)) {
                speechRecognizers[languageCode] = recognizer
                recognizer.delegate = self
            }
        }
        
        // è®¾ç½®é»˜è®¤è¯†åˆ«å™¨
        currentRecognizer = speechRecognizers[currentLanguage]
        print("æ”¯æŒçš„è¯†åˆ«å™¨: \(speechRecognizers.keys)")
    }
    
    // åˆ‡æ¢è¯†åˆ«è¯­è¨€
    func switchLanguage(to languageCode: String) {
        guard supportedLanguages.keys.contains(languageCode),
              let recognizer = speechRecognizers[languageCode] else {
            print("ä¸æ”¯æŒçš„è¯­è¨€: \(languageCode)")
            return
        }
        
        currentLanguage = languageCode
        currentRecognizer = recognizer
        print("åˆ‡æ¢åˆ°è¯­è¨€: \(supportedLanguages[languageCode] ?? languageCode)")
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œé‡æ–°å¼€å§‹ä»¥åº”ç”¨æ–°è¯­è¨€
        if isRecording {
            stopRecording()
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                self.startRecording()
            }
        }
    }
    
    // è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    func getSupportedLanguages() -> [String: String] {
        return supportedLanguages
    }
    
    // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
    func requestSpeechAuthorization() {
        print("å¼€å§‹è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™...")
        
        // é¦–å…ˆè¯·æ±‚éº¦å…‹é£æƒé™
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            DispatchQueue.main.async {
                if granted {
                    print("éº¦å…‹é£æƒé™å·²æˆæƒ")
                    // ç„¶åè¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
                    SFSpeechRecognizer.requestAuthorization { authStatus in
                        DispatchQueue.main.async {
                            switch authStatus {
                            case .authorized:
                                print("è¯­éŸ³è¯†åˆ«æƒé™å·²æˆæƒ")
                                self.isAuthorized = true
                                self.errorMessage = ""
                            case .denied:
                                print("è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»")
                                self.isAuthorized = false
                                self.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨è®¾ç½®ä¸­å¼€å¯"
                            case .restricted:
                                print("è¯­éŸ³è¯†åˆ«æƒé™å—é™")
                                self.isAuthorized = false
                                self.errorMessage = "è®¾å¤‡ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«æˆ–æƒé™å—é™"
                            case .notDetermined:
                                print("è¯­éŸ³è¯†åˆ«æƒé™æœªç¡®å®š")
                                self.isAuthorized = false
                                self.errorMessage = "è¯·æˆæƒè¯­éŸ³è¯†åˆ«æƒé™"
                            @unknown default:
                                print("è¯­éŸ³è¯†åˆ«æƒé™æœªçŸ¥çŠ¶æ€")
                                self.isAuthorized = false
                                self.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™çŠ¶æ€æœªçŸ¥"
                            }
                        }
                    }
                } else {
                    print("éº¦å…‹é£æƒé™è¢«æ‹’ç»")
                    self.isAuthorized = false
                    self.errorMessage = "éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œè¯·åœ¨è®¾ç½®ä¸­å¼€å¯"
                }
            }
        }
    }
    
    // å¼€å§‹å½•éŸ³å’Œè¯­éŸ³è¯†åˆ«
    func startRecording() {
        print("å¼€å§‹å½•éŸ³ï¼Œæƒé™çŠ¶æ€: \(isAuthorized), å½“å‰è¯­è¨€: \(currentLanguage)")
        
        guard isAuthorized else {
            errorMessage = "è¯·å…ˆæˆæƒè¯­éŸ³è¯†åˆ«å’Œéº¦å…‹é£æƒé™"
            return
        }
        
        guard let recognizer = currentRecognizer, recognizer.isAvailable else {
            errorMessage = "å½“å‰è¯­è¨€è¯†åˆ«å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            return
        }
        
        // é‡ç½®çŠ¶æ€
        if !isRecording {
            transcribedText = finalResults.joined(separator: " ") // ä¿ç•™æš‚åœå‰çš„æ–‡æœ¬
            errorMessage = ""
        }
        isRecording = true
        
        startSpeechRecognition()
    }
    
    func stopRecording() {
        print("ğŸ”´ [DEBUG] åœæ­¢å½•éŸ³å¼€å§‹...")
        print("ğŸ”´ [DEBUG] å½“å‰å½•éŸ³çŠ¶æ€: \(isRecording)")
        print("ğŸ”´ [DEBUG] å½“å‰è½¬å½•æ–‡å­—é•¿åº¦: \(transcribedText.count)")
        print("ğŸ”´ [DEBUG] å½“å‰è½¬å½•æ–‡å­—å†…å®¹: '\(transcribedText)'")
        print("ğŸ”´ [DEBUG] finalResults æ•°é‡: \(finalResults.count)")
        print("ğŸ”´ [DEBUG] finalResults å†…å®¹: \(finalResults)")
        
        if isRecording {
            // åœ¨åœæ­¢å‰ï¼Œå°†å½“å‰è½¬å½•æ–‡å­—ä¿å­˜åˆ° finalResults
            if !transcribedText.isEmpty && !finalResults.contains(transcribedText) {
                finalResults.append(transcribedText)
                print("ğŸ”´ [DEBUG] å°†å½“å‰è½¬å½•æ–‡å­—æ·»åŠ åˆ° finalResults: '\(transcribedText)'")
            }
            
            DispatchQueue.main.async {
                self.isRecording = false
                self.audioLevel = 0.0 // --- æ–°å¢ï¼šé‡ç½®éŸ³é‡ ---
                
                // ç¡®ä¿è½¬å½•æ–‡å­—ä¸è¢«æ¸…ç©ºï¼Œåˆå¹¶æ‰€æœ‰ç»“æœ
                let combinedText = self.finalResults.joined(separator: " ").trimmingCharacters(in: .whitespaces)
                if !combinedText.isEmpty {
                    self.transcribedText = combinedText
                    print("ğŸ”´ [DEBUG] åœæ­¢ååˆå¹¶çš„æ–‡å­—: '\(combinedText)'")
                } else {
                    print("ğŸ”´ [DEBUG] è­¦å‘Šï¼šåœæ­¢åæ²¡æœ‰ä»»ä½•æ–‡å­—å†…å®¹!")
                }
            }
            stopSpeechRecognition()
        }
        
        print("ğŸ”´ [DEBUG] åœæ­¢å½•éŸ³å®Œæˆï¼Œæœ€ç»ˆæ–‡å­—: '\(transcribedText)'")
    }
    
    // --- ä¼˜åŒ–ï¼šå°†æš‚åœå‰çš„ç»“æœåŠ å…¥ finalResults ---
    func pause() {
        if !transcribedText.isEmpty {
            finalResults.append(transcribedText)
            // æ¸…ç©ºå½“å‰æ–‡æœ¬ï¼Œé¿å…é‡å¤
            transcribedText = ""
        }
        stopRecording()
    }
    
    func clearText() {
        transcribedText = ""
        finalResults = []
        errorMessage = ""
    }
    
    // MARK: - Private Speech Recognition Logic
    
    private func startSpeechRecognition() {
        print("å¼€å§‹è¯­éŸ³è¯†åˆ«ï¼Œä½¿ç”¨è¯­è¨€: \(currentLanguage)")
        
        stopSpeechRecognition() // ç¡®ä¿ä¹‹å‰çš„ä»»åŠ¡å·²åœæ­¢
        
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            handleError("éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error.localizedDescription)")
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            handleError("æ— æ³•åˆ›å»ºè¯­éŸ³è¯†åˆ«è¯·æ±‚")
            return
        }
        
        // --- æ ¸å¿ƒä¼˜åŒ–ï¼šå¼€å¯è‡ªåŠ¨æ ‡ç‚¹å’Œè®¾ç½®ä»»åŠ¡ç±»å‹ ---
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.taskHint = .dictation // ä¼˜åŒ–é•¿è¯­éŸ³å¬å†™
        
        // iOS 16+ æä¾›æ›´ç²¾ç¡®çš„æ ‡ç‚¹æ§åˆ¶
        if #available(iOS 16, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        // å¯é€‰ï¼šæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥æé«˜ç‰¹å®šè¯æ±‡çš„è¯†åˆ«å‡†ç¡®ç‡
         recognitionRequest.contextualStrings = ["Adventure X", "knowhow"]
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // --- æ ¸å¿ƒä¼˜åŒ–ï¼šå®‰è£…éŸ³é¢‘ç›‘å¬å¹¶è®¡ç®—å®æ—¶éŸ³é‡ ---
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
            
            // --- æ–°å¢ï¼šè®¡ç®—éŸ³é‡ ---
            self?.updateAudioLevel(from: buffer)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
        } catch {
            handleError("å½•éŸ³å¯åŠ¨å¤±è´¥: \(error.localizedDescription)")
            return
        }
        
        recognitionTask = currentRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                // ä½¿ç”¨ result.bestTranscription.formattedString æ¥è·å–å¸¦æ ‡ç‚¹çš„æ–‡æœ¬
                let recognizedText = result.bestTranscription.formattedString
                let combinedText = (self.finalResults.joined(separator: " ") + " " + recognizedText).trimmingCharacters(in: .whitespaces)
                
                print("ğŸŸ¡ [DEBUG] è¯†åˆ«å›è°ƒ - isFinal: \(result.isFinal), è¯†åˆ«æ–‡å­—: '\(recognizedText)'")
                print("ğŸŸ¡ [DEBUG] è¯†åˆ«å›è°ƒ - åˆå¹¶åæ–‡å­—: '\(combinedText)'")
                
                DispatchQueue.main.async {
                    self.transcribedText = combinedText
                    print("ğŸŸ¡ [DEBUG] è¯†åˆ«å›è°ƒ - æ›´æ–°UIæ–‡å­—: '\(self.transcribedText)'")
                    
                    if result.isFinal {
                        // å°†æœ€ç»ˆç¡®è®¤çš„å®Œæ•´å¥å­å­˜å…¥ finalResults
                        self.finalResults.append(recognizedText)
                        // é‡ç½® transcribedTextï¼Œç­‰å¾…ä¸‹ä¸€å¥
                        self.transcribedText = self.finalResults.joined(separator: " ")
                        print("ğŸŸ¡ [DEBUG] è¯†åˆ«å›è°ƒ - æœ€ç»ˆç»“æœï¼Œæ›´æ–°finalResults: \(self.finalResults)")
                        print("ğŸŸ¡ [DEBUG] è¯†åˆ«å›è°ƒ - æœ€ç»ˆç»“æœï¼Œé‡ç½®åæ–‡å­—: '\(self.transcribedText)'")
                    }
                }
            }
            
            if let error = error {
                print("ğŸ”´ [DEBUG] è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                print("ğŸ”´ [DEBUG] é”™è¯¯æ—¶å½“å‰æ–‡å­—: '\(self.transcribedText)'")
                print("ğŸ”´ [DEBUG] é”™è¯¯æ—¶finalResults: \(self.finalResults)")
                self.stopRecording()
                // å¯ä»¥æ ¹æ® error code æä¾›æ›´å‹å¥½çš„æç¤º
            }
        }
    }
    
    private func stopSpeechRecognition() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        recognitionTask?.cancel()
        recognitionTask = nil
    }
    
    // --- æ–°å¢ï¼šå®æ—¶éŸ³é‡è®¡ç®—å’Œæ ‡å‡†åŒ– ---
    private func updateAudioLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData else { return }
        
        let channelDataValue = channelData.pointee
        let channelDataValueArray = UnsafeBufferPointer(start: channelDataValue, count: Int(buffer.frameLength))
        
        let rms = sqrt(channelDataValueArray.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
        let avgPower = 20 * log10(rms)
        
        // å°†åˆ†è´å€¼æ ‡å‡†åŒ–åˆ° 0-1 çš„èŒƒå›´
        let minDb: Float = -80.0 // é™éŸ³æ—¶çš„åˆ†è´å€¼
        let maxDb: Float = -10.0  // è¾ƒå¤§éŸ³é‡æ—¶çš„åˆ†è´å€¼
        
        var normalizedLevel = (avgPower - minDb) / (maxDb - minDb)
        normalizedLevel = max(0.0, min(1.0, normalizedLevel)) // é™åˆ¶åœ¨ 0-1 ä¹‹é—´
        
        DispatchQueue.main.async {
            // ä½¿ç”¨å¹³æ»‘è¿‡æ¸¡ï¼Œé¿å…UIè·³åŠ¨
            self.audioLevel = (self.audioLevel * 0.8) + (normalizedLevel * 0.2)
        }
    }
    
    private func handleError(_ message: String) {
        print(message)
        DispatchQueue.main.async {
            self.errorMessage = message
            self.isRecording = false
            self.audioLevel = 0.0
        }
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension SpeechRecognitionManager: SFSpeechRecognizerDelegate {
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        DispatchQueue.main.async {
            if !available && self.isRecording {
                self.errorMessage = "è¯­éŸ³è¯†åˆ«æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                self.stopRecording()
            } else if available {
                self.errorMessage = ""
            }
            
            print("è¯­éŸ³è¯†åˆ«å™¨å¯ç”¨æ€§å˜åŒ–: \(available)")
        }
    }
}
